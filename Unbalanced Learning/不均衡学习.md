# 不均衡学习

## 1. 何为不均衡学习？

通常二分类机器学习任务期望两种类别的样本是均衡的，即两类样本的总量接近相同。因为在梯度下降过程中，不同类别的样本量有较大差异时，很难收敛到最优解。但在很多真实场景下，数据集往往是不平衡的。也就是说，在数据集中，有一类含有的数据要远远多于其他类的数据。尤其是在风控场景下，负样本的占比要远远小于正样本的占比。

考虑一个简单的例子，假设有10万个正样本（正常客户，标签为0）与1000个负样本（欺诈客户，标签为1），正负样本比例为100:1。如果直接送入模型中去学习，每一次梯度下降都使用全量样本，其中负样本所贡献的信息只有模型接收到的总信息的1/100，不能保证模型能很好地学习负样本。所以，需要一个分类器，既能有效地学习正样本的信息，同时又不会影响到负样本的学习。

下面总结几类解决样本不均衡问题的方法。

## 2.欠采样

### 2.1 何为欠采样

欠采样就是对训练集中多数类样本进行下采样，即去除一部分多数类中的样本使得正例、反例数目接近，然后再进行学习。

### 2.2 随机欠采样

随机欠采样顾名思义就是从多数类中随机选择一些样本从中移除。

缺点：

随机欠采样方法通过改变多数类样本比例以达到修改样本分布的目的，从而使样本分布较为均衡，但是这也存在一些问题。对于随机欠采样，由于采样的样本集合要少于原来的样本集合，因此会造成一些信息缺失，即将多数类样本删除有可能会导致分类器丢失有关多数类的重要信息。

为了克服随机欠采样方法导致的信息缺失问题，又要保证算法表现出较好的不均衡数据分类性能，出现了欠采样方法的代表性算法EasyEnsemble和BalanceCascade算法。

### 2.3 EasyEnsemble

（1）算法思想



（2）算法步骤



### 2.4 BalanceCascade

（1）算法思想



（2）算法步骤



## 3.过采样





## 4.代价敏感学习





## 5.半监督学习





## 6.不均衡学习的评价方法

### 6.1 F1



### 6.2 G-Mean



### 6.3 AUC



