# 噪声、共线性、太多的特征对gbdt的负面影响

说说最常见的，共线性问题，特征相关性太高引发的问题：

**1、特征重要性的有效性变差**，比如某个强特本来特征重要性是很高的，但是因为存在多组相关性很强的强特，极端的例子，直接赋值几列取值完全相同的特征列，则在feature importance中，这个强特的特征重要性将被稀释；具体测试案例可见：

**2、共线性会影响模型的泛化性能**

[Quora Question Pairs | Kagglewww.kaggle.com](https://link.zhihu.com/?target=https%3A//www.kaggle.com/c/quora-question-pairs/discussion/33876)

![img](https://pic3.zhimg.com/80/v2-7527d32ebe1c96de91d60d538e16461a_720w.jpg)

正如这个作者所说，太多共线性特征的存在确实会使得模型的泛化性能下降，当然，这里指的是“过多”，如果是很少量共线性特征存在其实对于模型影响很轻微，但是如果存在大量共线性特征，尤其是比赛的时候暴力的特征衍生，会产生相当多相关性很高的特征，从而导致gbdt在训练的过程中重复采样相关性很高的特征，使得模型的效果变差，具体可见kaggle_ieee的kris分享的方案，通过删除大量冗余的V特征，local cv上升了千五，b榜上涨千4（事后分别测试的）。

补充：针对不少人的提问，这里补充一下吧，xgb或lgb的泛化性能好的重要原因之一（注意是之一，下面所说的只是其中的一方面）可以通过行列采样构建不同的特征空间和样本空间下的基模型，集成学习的核心“好而不同”，xgb或lgb解决了“不同”的问题，至于单树的“好”或者“坏”在gbm的框架下影响并不大，单棵树的表达能力不够，多点树来凑，具体的应用中的例子就是，对于过拟合问题，设置大一点的行列采样比例则过拟合的程度很快就能降下来，可以行列采样和最大深度对于gbdt整体模型的影响是比较显著的，立竿见影。 而之所以共线性会影响泛化性能，前提存在大量的共线性，比如有10个特征 A、B。。。。J，假设这10个特征完全独立，并且假设我们每次列采样比例为0.2，也就是每次采样2个特征出来训练一个基模型，则一共有10*9/2=45种组合，这个时候，我们再假设一个极端的情况，B~J的特征完全一样，是相同特征的复制，这个时候，不管我们怎么采样永远只有两种特征的组合，即A和（B~J）其中任何一个特征的组合都是一样的或者（B~J）内部的特征组合，显然，基模型的可以使用的子空间的种类大大降低，集成模型的所谓“不同”大大弱化了，多样性降低，自然模型的泛化性能下来了。



**3、不重要的（噪声）特征是否需要删除？gbdt算法的分裂的过程中可以自动做到特征选择那么是否意味着我可以无限做特征衍生，反正模型会自动筛选其中的好坏特征？**

首先，暴力衍生不太可取，一方面太耗时间和内存，一方面容易产生高相关性（或者高局部相关的）特征与噪声，高相关性特征可以通过相关系数来剔除，但是高局部相关特征的检验很复杂并且麻烦，而噪声特征的处理也比较繁琐耗时。

> 噪声变量不会带来大的增益，所以实际上树在分裂的时候不会考虑他们从而也不会影响树的分裂，因此无限大的特征维度对tree 没有影响

回答：这仅对于非常巨大的的数据集是正确的，因为大量的训练集中的样本数量可以很好地涵盖所有特征空间由于引入新特征而产生的变化。但是在实践中，样本数量往往有限，如果特征维数足够大（比如暴力的特征衍生），最终会带来很多采样噪声，因为数据越多维数，对可能的分布空间的覆盖范围的能力就越弱。

**最终与标签偶然相关联的弱变量上的噪声可能会限制增强算法的有效性**，而这在决策树中的更深的深度的拆分中更容易发生，在该决策树中，已评估的数据已被分组为一个小的子集。

**添加的变量越多，弱相关变量就越有可能恰好适合某些特定组合的分割选择**，然后创建新的分支，但是这是噪声产生的模型分支对于未来的预测有很大偏误。

实际上，我发现XGBoost在小范围内对噪声非常鲁棒。但是，我也发现，出于类似的原因，它有时会选择质量较差的特征变量，而不是关联性更好的数据。**因此，这不是“变量越多对XGBoost越好”的算法，您确实需要考虑可能的低质量功能。**

1. 更多内存占用，更慢的装载速度，更慢的处理速度等。
2. 每个功能需要最少数量的样本才能有效学习模型。通过包含冗余特征，此样本：特征比率得以减小，这使许多模型更难确定特征是否是有用的预测因子。（补充一下，这里作者的意思是这样的，就是假设存在过多的垃圾特征，那么树在分裂的过程中难免会用到这些垃圾特征导致树的分裂，继而导致每个新的分支的样本数量的减少，那么比如我们设置叶子节点最小样本数量、最大叶子节点数量等超参数的时候就很容易导致树因为达到了预设条件而提前终止分裂，退一步说，即使没有超参数的限制，因为被垃圾特征“抢走”了不少分裂的样本，可能导致真正有效的特征最终只能在少数样本上分裂，这将导致模型无法正确评估这类实际上有效的特征并且也无法正确利用有效的特征）。

另外补充一点，关于噪声特征，我们可以使用kris验证法的变体来测试，大体上分为两种，一种是完全的废物特征，在训练集和验证集上的得分都很低，这类特征少量存在其实gbdt是可以很好的handle的，但是当这类特征大量存在的时候非常影响模型的性能，比如我有两个强特和2000个垃圾特征，一起训练和剔除垃圾特征之后训练的结果差异性会相当之大的；一种是“伪装的强特”，在训练集上得分很高而在测试集上得分很低，这类特征仅仅有少数就能够影响模型的泛化性能了。

关于kris验证方法的变体大概形式如下：

![img](https://pic1.zhimg.com/80/v2-42da39f86eb2b3a7810c3ea5bd69acc0_720w.jpg)

实际上就是每个特征单独跑个交叉验证，参数尽量宽松设置以最大化特征的“潜能”，通过交叉验证的平均训练集与验证集得分来判定特征属于哪一种特征。这种方法的好处在于可以很直观而独立的了解每一个特征的偏移情况，不足之处在于无法确定哪一些“特征组合”的偏移情况，因为我们需要知道的是，关于特征偏移，实际上很多时候都是和特征的组合相关的而不是单特征相关（当然偏移很厉害的单特征也是存在这里只是提出一种更常见的现象），比如上面这些特征，单个特征的偏移程度可能都不会非常大，最大的大概就5个点auc，但是我们将两个偏移3个点的特征一起入模计算就会发现偏移可能会到达9~10个点，因此针对这类现象，我们可以对偏移严重的特征进行组合的穷举然后两个两个特征入模去计算从而得出哪些特征组合的偏移严重，不过很多时候单个特征的问题能够解决，高阶特征组合的问题自然而然也能得到解决，后续会把前段时间磕磕绊绊的ieee的总结结果搬上来，感觉多看看大佬的打比赛的思路还是很有收获的。



https://zhuanlan.zhihu.com/p/105094131

