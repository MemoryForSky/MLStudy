# auc和accuracy

## 1.auc和accuracy的计算逻辑

首先，accuracy高，AUC不一定高，相反，accuracy低，AUC不一定低，这个可以说没有内在关系。

auc代表的是**分类**或者**排序**能力，与分类阈值无关；而accuracy是和**阈值**有关的。

以Binary Classification为例。

首先，AUC对应的不是一个accuracy，而是一系列accuracy。AUC是ROC的"线下面积"，而ROC是以FPR-TPR为坐标的一条线，实际上是连接一系列散点的一条折线。这条折线上的每一个点，对应了一个threshold，以及由这个threshold确定的预测值及其accuracy、precision、recall等等的度量。

所以说，AUC衡量的是一个模型的好坏，是它给所有样本排序的合理程度（是不是正确地把负例排在了正例的前面）；而accuracy衡量的是一个模型在一个特定threshold（比如，logistic regression模型在阈值0.5）下的预测准确度（是不是正确地把负例排在了阈值之前，正例排在了阈值之后）。

因此，一个模型定了，它的AUC就定了。但我可以取一个threshold，使得它的accuracy尽量低或者尽量高（有上限和下限）。

## 2.auc和accuracy的关系

- auc高，accuracy低：可能的原因是分类阈值的选择。极端来讲，默认阈值是0.5，但模型输出的值全部小于0.5，那正样本是全错的；但由于auc很高，正负样本还是可以分开，把分类阈值调小即可。

- auc低，accuracy高：这种一般发生在分布不平衡的问题中。比例少的那类分类错误很多，但由于数量少整体的accuracy还是很高，但代表分类能力的auc就会很低，

## 3.如何选择评价指标

关于选择什么评价指标，由你的问题决定的。比如，你做排序，最后模型出来的结果是一系列实数值(评分模型，比如芝麻信用评分)，以芝麻信用分为例，你自然希望这些值表示“越大信用越好”，怎么证明你预测的这个信用分是可靠的呢？这时候AUC就派上用场了。原因在于，AUC可以看做随机从正负样本中选取一对正负样本，其中正样本的得分大于负样本的概率！

而accuracy呢，只有在正负样本非常平衡的时候才比较有意义。如果不平衡，比如有100个样本，99个正的，一个负的，我随便拿个模型，来预测，正确率都99%,显然毫无意义。

在此，另外补充一点，关于评价指标选取的重要性，有时候不是我们没做出好的模型，而是我们没选择对正确的评价指标。以查准率与查全率为例，如果你的模型是关注“所有的小偷中有多少被抓到”这样的问题，那么你就会在乎查全率；如果你的模型关注“所有抓住的嫌疑犯中，有多少个是真的小偷”，那你就应该用查准率。所以，可以看到，问题不同，评价指标还是很不一样的。

综上，在机器学习实际应用的过程中，有很多地方都要求我们结合实际业务来做出选择与判断，只有更好地理解机器学习，只有更好地理解模型的特点，我们才能最终做出能够落地的模型。



