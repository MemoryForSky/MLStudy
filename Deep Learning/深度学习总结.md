# 深度学习总结

## 1.机器学习框架

### 1.1 Framework of ML

- Step1: function with unknown parameters
- Step2: define loss from training data
- Step3: optimization

![image-20210419230033272](./img/dl1.png)

### 1.2 General Guide

![image-20210419231759919](./img/dl2.png)

（1）Model Bias   VS   Optimization Issue

![image-20210419232053867](./img/dl3.png)

![image-20210419232149290](./img/dl4.png)

![image-20210419232302818](./img/dl5.png)

（2）Overfitting

![image-20210419232629089](./img/dl6.png)

![image-20210419232739444](./img/dl7.png)

（3）Bias Complexity Trade off

![image-20210419232833308](./img/dl8.png)

（4）Cross Validation

![image-20210419234521697](./img/dl9.png)

![image-20210419234615307](./img/dl10.png)

（5）mismatch

![image-20210419234729237](./img/dl11.png)

## 2.模型为什么不work？

### 2.1 局部最小值 (local minima) 与鞍点 (saddle point)

![image-20210419235549391](./img/dl12.png)

数学证明：

![image-20210420000624426](./img/dl13.png)

![image-20210420000713238](./img/dl14.png)

![image-20210420000753380](./img/dl15.png)

![image-20210420002833826](./img/dl16.png)

通过一个实验可以看出：实际在高维空间，local minima并不常见，更多见到的是saddle point，所以梯度为0的时候，是可以解决梯度下降的问题的。

### 2.2 Batch and Momentum

对抗local minima和saddle point

（1）Batch

![image-20210420222814915](./img/dl17.png)

（2）为什么要用Batch？  

更快

![image-20210420232349786](D:\develop\github\MLStudy\Deep Learning\img\dl18.png)

![image-20210420232425435](D:\develop\github\MLStudy\Deep Learning\img\dl19.png)

更好

![image-20210420232537604](D:\develop\github\MLStudy\Deep Learning\img\dl20.png)

![image-20210420232620068](D:\develop\github\MLStudy\Deep Learning\img\dl21.png)

![image-20210420232849717](D:\develop\github\MLStudy\Deep Learning\img\dl22.png)

![image-20210420233058412](D:\develop\github\MLStudy\Deep Learning\img\dl23.png)

![image-20210420233247045](D:\develop\github\MLStudy\Deep Learning\img\dl24.png)

（3）Momentum

![image-20210420234043466](D:\develop\github\MLStudy\Deep Learning\img\dl25.png)

![image-20210420234143168](D:\develop\github\MLStudy\Deep Learning\img\dl26.png)

![image-20210420234218507](D:\develop\github\MLStudy\Deep Learning\img\dl27.png)

![image-20210420234248180](D:\develop\github\MLStudy\Deep Learning\img\dl28.png)

![image-20210420234346550](D:\develop\github\MLStudy\Deep Learning\img\dl29.png)

###  2.3 Adaptive Learning Rate

Error surface is rugged ...

![image-20210420235637124](D:\develop\github\MLStudy\Deep Learning\img\dl30.png)

![image-20210421003200726](D:\develop\github\MLStudy\Deep Learning\img\dl31.png)

![image-20210421003918027](D:\develop\github\MLStudy\Deep Learning\img\dl32.png)

![image-20210421004001933](D:\develop\github\MLStudy\Deep Learning\img\dl33.png)

![image-20210421004045679](D:\develop\github\MLStudy\Deep Learning\img\dl34.png)

![image-20210421004302107](D:\develop\github\MLStudy\Deep Learning\img\dl35.png)

![image-20210421004347498](D:\develop\github\MLStudy\Deep Learning\img\dl36.png)

![image-20210421004418598](D:\develop\github\MLStudy\Deep Learning\img\dl37.png)

![image-20210421004504107](D:\develop\github\MLStudy\Deep Learning\img\dl38.png)

![image-20210421004817103](D:\develop\github\MLStudy\Deep Learning\img\dl39.png)

![image-20210421004923679](D:\develop\github\MLStudy\Deep Learning\img\dl40.png)

![image-20210421005700263](D:\develop\github\MLStudy\Deep Learning\img\dl41.png)

![image-20210421005729254](D:\develop\github\MLStudy\Deep Learning\img\dl42.png)

![image-20210421005802944](D:\develop\github\MLStudy\Deep Learning\img\dl43.png)

![image-20210421005831328](D:\develop\github\MLStudy\Deep Learning\img\dl44.png)

### 2.4 損失函數 (Loss) 

![image-20210421230710328](D:\develop\github\MLStudy\Deep Learning\img\dl45.png)

![image-20210421230757784](D:\develop\github\MLStudy\Deep Learning\img\dl46.png)

### 2.5  批次标准化 (Batch Normalization) 



















































