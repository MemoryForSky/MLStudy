# 迁移学习

## 1. 何为迁移学习

### 1.1 迁移学习从何而来

由于建立模型必须有一定的数据积累，但是在项目冷启动时，很多时候没有足够的数据。

 不过我们手头会有一些其他领域的标注数据和模型，这就使得对已有标签的数据和模型进行重用成为可能。传统机器学习方法通常假定这些数据服从相同分布，因此不再适用。

如何基于已有的不同分布数据，快速构建模型，实现数据标定，是一个重要问题。

通过迁移学习算法，就可以让不同领域的知识互相借鉴，实现建模的目标。

### 1.2 何为迁移学习

（1）迁移学习

通过减小源域（辅助领域）到目标域的**分布差异**，进行**知识迁移**，从而实现数据标定。

![image-20210330232622513](D:\develop\github\MLStudy\Transfer Learning\img\tl1.png)

> 迁移学习是机器学习领域用于解决**标记数据难获取**这一基础问题的重要手段。

（2）核心思想

- 找到不同任务之间的**相关性**
- 举一反三、照猫画虎，但不要东施效颦（负迁移）

### 1.3 应用场景

迁移学习的应用前景广阔，包括模式识别、计算机视觉、语音识别、自然语言处理、数据挖掘等。

![image-20210330232805932](D:\develop\github\MLStudy\Transfer Learning\img\tl2.png)

风控领域中会用到迁移学习的常见场景有：

- 新开了某个消费分期的场景，由于只有少量样本，需要用其他场景的数据来建模。此时其他场景数据为源域，新消费分期场景为目标域。
- 业务被迫停止3个月后项目重启，大部分训练样本比较老旧，新的训练样本又不够。此时大部分旧样本为源域，新的少量训练样本为目标域。
- 在某个国家新开展了类似国内的业务，因为国情不同，部分特征分布也是不同的。此时有数据积累的国内数据为源域，新国家场景为目标域。

类似的场景还有很多，都可以通过迁移学习将其他场景的存量数据带入模型进行训练，通过功能不同的模型，对特征进行映射或挑选更符合目标场景的旧样本。

一句话概括：**源域样本和目标域样本分布有区别，目标域样本量又不够。**

### 1.4 为什么需要迁移学习

（1）数据角度

- 收集数据很困难
- 为数据打标签很耗时
- 训练一对一的模型很繁琐

（2）模型角度

- 个性化模型很复杂
- 云+端的模型需要作具体化适配

（3）应用角度

- 冷启动问题：没有足够用户数据，推荐系统无法工作

## 2. 迁移学习方法论

### 2.1 方法分类

（1）基于**实例**的迁移（instance based TL）

- 通过权重重用源域和目标域的样例进行迁移

（2）基于**特征**的迁移（feature based TL）

- 将源域和目标域的特征变换到相同空间

（3）基于**模型**的迁移（parameter based TL）

- 利用源域和目标域的参数共享模型

（4）基于**关系**的迁移（relation based TL）

- 利用源域中的逻辑网络关系进行迁移

其中，基于实例、特征和模型的迁移较为常用，基于关系的迁移使用较少。

### 2.2 研究领域和方法分类

![image-20210330234029639](D:\develop\github\MLStudy\Transfer Learning\img\tl3.png)

## 3. 域自适应

### 3.1 域自适应问题

（1）几个基本概念

- **域(Domain)**：由数据特征和特征分布组成，是学习的主体。
  - 源域(Source domain)：已有知识的域
  - 目标域(Target domain)：要进行学习的域
- **任务(Task)**：有目标域和学习结果组成，是学习的结果。

（2）问题定义

- 条件：给定一个源域$D_S$和源域上的学习任务$T_S$，目标域$D_T$和目标域上的学习任务$T_T$。
- 目标：利用$D_S$和$T_S$学习在目标域上的预测函数$f(·)$。
- 限制条件：$D_S\not = D_T$或$T_S\not = T_T$。

（3）域自适应问题

- Domain Adaptation(DA)；cross-domain learning；同构迁移学习。

- 问题定义：有标签的源域和无标签的目标域共享相同的特征和类别，但是**特征分布不同**，如何利用源域标定目标域。

  $$D_S \not = D_T: P_S(X) \not = P_T(X)$$

- 按照目标域有无标签

  - 目标域全部有标签：supervised DA
  - 目标域有一些标签：semi-supervised DA
  - 目标域全没有标签：unsupervised DA

下面主要针对无监督迁移学习方法作简要概述。

### 3.2 域自适应方法

（1）基本假设

- 数据分布角度：源域和目标域的**概率分布相似**
  - **最小化**概率分布距离
- 特征选择角度：源域和目标域共享着**某些特征**
  - **选择**出这部分公共特征
- 特征变换角度：源域和目标域共享**某些子空间**
  - 把两个域变换到相同的子空间

（2）解决思路

基于三种不同的角度，可以考虑三种解决方法：

- 概率分布适配法（Distribution Adaptation）
- 特征选择法（Feature Selection）
- 子空间学习法（Subspace Learning）

![image-20210331000440620](D:\develop\github\MLStudy\Transfer Learning\img\tl4.png)

### 3.3 概率分布适配法

对于概率分布适配法，根据源域和目标域分布的差异，可以考虑以下三种概率适配方法：

- 边缘分布适配（Marginal distribution adaptation）
  - 假设：$P(X_s) \not = P(X_t)$
- 条件分布适配（Conditional distribution adaptation）
  - 假设：$P(y_s|X_s) \not = P(y_t|X_t)$
- 联合分布适配（Joint distribution adaptation）
  - 假设：$P(X_s, y_s) \not = P(X_t, y_t)$

简单来说，数据的边缘分布不同，就是数据整体不相似。数据的条件分布不同，就是数据整体相似，但是具体到每个类里，都不太相似。数据联合分布不同，就是数据的整体和每个类都不太相似，需要同时考虑边缘分布和条件分布适配，即联合分布适配。

![image-20210331001245817](D:\develop\github\MLStudy\Transfer Learning\img\tl5.png)

#### 3.3.1 边缘分布适配

![image-20210331001745927](D:\develop\github\MLStudy\Transfer Learning\img\tl6.png)

![image-20210331002006661](D:\develop\github\MLStudy\Transfer Learning\img\tl7.png)

#### 3.3.2 条件分布适配

![image-20210331002219888](D:\develop\github\MLStudy\Transfer Learning\img\tl8.png)

#### 3.3.3 联合分布适配

联合分布适配方法效果最好，优先使用。

![image-20210331002329344](D:\develop\github\MLStudy\Transfer Learning\img\tl9.png)

![image-20210331002655280](D:\develop\github\MLStudy\Transfer Learning\img\tl10.png)

![image-20210331003021574](D:\develop\github\MLStudy\Transfer Learning\img\tl11.png)

![image-20210331003333328](D:\develop\github\MLStudy\Transfer Learning\img\tl12.png)

![image-20210331003421609](D:\develop\github\MLStudy\Transfer Learning\img\tl13.png)

### 3.4 特征选择法

![image-20210331003912452](D:\develop\github\MLStudy\Transfer Learning\img\tl14.png)

![image-20210331004034059](D:\develop\github\MLStudy\Transfer Learning\img\tl15.png)

![image-20210331004237754](D:\develop\github\MLStudy\Transfer Learning\img\tl16.png)

### 3.5 子空间学习法

![image-20210331004341128](D:\develop\github\MLStudy\Transfer Learning\img\tl17.png)

#### 3.5.1 统计特征变换

![image-20210331004544634](D:\develop\github\MLStudy\Transfer Learning\img\tl18.png)

![image-20210331010908757](D:\develop\github\MLStudy\Transfer Learning\img\tl19.png)

#### 3.5.2 流形学习

![image-20210331011028967](D:\develop\github\MLStudy\Transfer Learning\img\tl20.png)

![image-20210331011350036](D:\develop\github\MLStudy\Transfer Learning\img\tl21.png)

![image-20210331011434694](D:\develop\github\MLStudy\Transfer Learning\img\tl22.png)

### 3.6 最新研究成果

![image-20210331011551136](D:\develop\github\MLStudy\Transfer Learning\img\tl23.png)

![image-20210331011818035](D:\develop\github\MLStudy\Transfer Learning\img\tl24.png)

![image-20210331011957228](D:\develop\github\MLStudy\Transfer Learning\img\tl25.png)

![image-20210331012150162](D:\develop\github\MLStudy\Transfer Learning\img\tl26.png)

## 4. 少量有标签样本迁移算法

TrAdaBoost算法用来解决训练集和测试集分布不同的问题。在迁移学习的某些情况下，一个训练集中会包含大量的源域训练样本和少量的目标域训练样本。通常建模人员会将两个不同分布的训练集放在一起训练，这种方法也被称为基于**实例**的迁移学习方法。



## 5. 无标签样本迁移算法

### 5.1 JDA



### 5.2 DTELM 



## 6. 迁移样本筛选方案



## 7. 迁移学习在深度学习中的应用

迁移学习在深度学习中的应用，使用较为广泛的是基于模型的迁移，比如fine-tuning。













